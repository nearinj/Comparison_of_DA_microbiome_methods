---
title: "Concordance_Correlation_Analysis"
author: "Jacob T. Nearing"
date: "7/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reshape2)
library(ggplot2)
library(ffpe)
library(doParallel)
library(stringr)

display_items_out <- "/home/jacob/GitHub_Repos/Clean_Hackathon/Display_items/"



tool_names <- c(aldex2="ALDEx2", ancom="ANCOM-II", corncob="corncob", deseq2="DESeq2", edger="edgeR", lefse="LEfSe", 
                limma_voom_TMM="LV TMM", limma_voom_TMMwsp="LV TMMwsp", maaslin2="MAL2",
                maaslin2rare="MAL2 rare", metagenomeSeq="metagenom.", ttestrare="t rare", 
                wilcoxonclr="Wilcox CLR", wilcoxonrare="Wilcox rare")
```

# Load in data

```{r}

filt_results <- readRDS("/home/jacob/GitHub_Repos/Hackathon_testing/Data/Filt_results_21_04_07.RDS")
unfilt_results <- readRDS("/home/jacob/GitHub_Repos/Hackathon_testing/Data/Unfilt_results_21_04_07.RDS")

unfilt_study_tab  <- readRDS("/home/jacob/GitHub_Repos/Hackathon_testing/Data/unfilt_study_tab_21_04_07.RDS")
filt_study_tab <- readRDS("/home/jacob/GitHub_Repos/Hackathon_testing/Data/filt_study_tab_21_04_07.RDS")

```

#Concordance Analysis
```{r}


#function that takes in a p-val table and caluclates concordance of 
get_concordance_of_p <- function(p_val_tab, maxrank=2000){
  
  #we need to do all the pairwise concordance values for this dataset
  concord_res <- list()
  #go through the cols
  for(i in 1:length(colnames(p_val_tab))){
    tool <- colnames(p_val_tab)[i]
    #vec1 represents the p-value vector for first tool
    vec1 <- p_val_tab[,i]
    #add names to vector
    names(vec1) <- rownames(p_val_tab)
    
    for(j in 1:length(colnames(p_val_tab))){
      
      if(i!=j){
        tool2 <- colnames(p_val_tab)[j]
        vec2 <- p_val_tab[,j]
        names(vec2) <- rownames(p_val_tab)
        
        concord <- ffpe::CATplot(vec1=vec1, vec2 = vec2, maxrank=maxrank, make.plot=T)
        return(concord)
        #sort the tool names in alphabetical order
        #in some cases we will overwrite when we run the test twice.
        #this is fine for now as I'm not super focused on speed.
        comp_name <- sort(c(as.character(tool), as.character(tool2)))
        concord_res[[paste(comp_name[1], comp_name[2], sep='+')]] <- concord
      }
    }
  }
  return(concord_res)
  
}






get_concord_auc <- function(concord_list, maxrank=2000){
  
  #take in a list of concordance vectors
  AUC_res <- list()
  
  for(i in 1:length(concord_list)){
    comp_name <- names(concord_list)[i]
    auc <- MESS::auc(x=1:maxrank, y=concord_list[[i]]$concordance)
    AUC_res[[comp_name]] <- auc
  }
  
  return(AUC_res)
  
}

get_concord_auc_diff <- function(concord_list, maxrank=2000, n_feats){
  
  AUC_res <- list()
  
  for(i in 1:length(concord_list)){
    comp_name <- names(concord_list)[i]
    auc_real <- MESS::auc(x=1:maxrank, y=concord_list[[i]]$concordance)
    auc_expect <- MESS::auc(x=1:maxrank, y=(1:maxrank)/n_feats)
    diff <- auc_real - auc_expect
    AUC_res[[comp_name]] <- diff
  }
  
}
```

## Testing
```{r}
test_p_vals <- data.frame(matrix(1:100, 100, 10))
colnames(test_p_vals) <- seq(1:10)
test_p_vals[,2] <- c(100:1)
test_p_vals[,3] <- sample(1:100, 100, replace=T)
test_p_vals[,4] <- c(1:25,100:26)
test_p_vals_res <- get_concordance_of_p(test_p_vals, maxrank=50)
test_p_vals_res$concordance
names(test_p_vals_res)

#should be 0
test_p_vals_res[[1]]

#should be random value
test_p_vals_res[[2]]

#should be half at 50 and 1 up until 25
test_p_vals_res[[3]]
# works as expected

test_auc <- get_concord_auc(test_p_vals_res, maxrank=50)
test_auc[[2]]

melt_test_auc <- melt(test_auc)

```
## Get concord results
```{r}
cl <- makeCluster(38)
registerDoParallel(cl)

concord_filt_res <- foreach(i=1:length(filt_results)) %dopar%{
  get_concordance_of_p(filt_results[[i]][[2]], maxrank = 100)
}
names(concord_filt_res) <- names(filt_results)
stopCluster(cl)
saveRDS(concord_filt_res, "~/GitHub_Repos/Clean_Hackathon/Data/filt_concord_res.RDS")

cl <- makeCluster(38)
registerDoParallel(cl)

concord_unfilt_res <- foreach(i=1:length(unfilt_results)) %dopar%{
  get_concordance_of_p(filt_results[[i]][[2]], maxrank = 100)
}
names(concord_unfilt_res) <- names(unfilt_results)
stopCluster(cl)
saveRDS(concord_unfilt_res, "~/GitHub_Repos/Clean_Hackathon/Data/unfilt_concord_res.RDS")

```


## Concord Analysis
```{r}
#easiest thing to do is calualte the AUC of each concord vector
AUC_res_filt <- list()

for(i in 1:length(concord_filt_res)){
  
  dataset_name <- names(concord_filt_res)[i]
  message(dataset_name)
  AUC_res_filt[[dataset_name]] <- get_concord_auc(concord_filt_res[[i]], maxrank = 100)
}

#go through results list and melt them into a single DF for plotting...
AUC_res_filt_melt <- list()
for(i in 1:length(AUC_res_filt)){
  dataset_name <- names(AUC_res_filt)[i]
  temp_melt <- melt(AUC_res_filt[[i]])
  split_data <- str_split_fixed(temp_melt$L1, "\\+", 2)
  temp_melt <- cbind(temp_melt, split_data)
  AUC_res_filt_melt[[i]] <- temp_melt
}


AUC_res_filt_melt_comb <- do.call(rbind, AUC_res_filt_melt)
colnames(AUC_res_filt_melt_comb) <- c("value", "comb", "Var1", "Var2")

#fix tool names
AUC_res_filt_melt_comb$Var1 <- tool_names[as.character(AUC_res_filt_melt_comb$Var1)]
AUC_res_filt_melt_comb$Var2 <- tool_names[as.character(AUC_res_filt_melt_comb$Var2)]

## calculate means

mean_vals <- aggregate(AUC_res_filt_melt_comb$value, by=list(AUC_res_filt_melt_comb$comb), FUN=mean)
matching_vals <- match(AUC_res_filt_melt_comb$comb, mean_vals$Group.1)
AUC_res_filt_melt_comb$mean <- mean_vals$x[matching_vals]

## remove ANCOM..
ancom_values <- grep("ancom",AUC_res_filt_melt_comb$comb)
ancom_values


#remove ancom values from table
remove_ancom_vals <- AUC_res_filt_melt_comb[-ancom_values,]

AUC_plot <- ggplot(remove_ancom_vals, aes(y=value, fill=mean)) +
  geom_rect(xmin= -Inf, xmax=Inf, ymin=-Inf,ymax=Inf) + geom_boxplot() + facet_grid(Var2 ~ Var1) + scale_x_discrete(element_blank()) +
    scale_fill_gradient2(low="blue", mid="white", high="red", name="Concordance AUC") + ylab("Concordance AUC")
AUC_plot
```
#Correlation Analysis

So we could do two things.... We could plot mean correlation over all datasets or we could combined all the datasets together and get one large correlation

I think I like the inital idea for now.
## Functions
```{r}

#will include a rank cutoff as well
get_tool_correlations <- function(p_tab_results, method="pearson"){
  
  #go through each dataset and calculate the pairwise correlation
  cor_matrices <- list()
  for(i in 1:length(p_tab_results)){
    
    dataset_name <- names(p_tab_results)[i]
    
    
    
    # we need to get pairwise correlations
    dataset_cor_matrix <- cor(p_tab_results[[i]]$adjP_table, use="pairwise.complete.obs", method=method)
    cor_matrices[[dataset_name]] <- dataset_cor_matrix
  }
  
  return(cor_matrices)
}


get_melted_cor_df <- function(cor_res){
  
  res_results <- data.frame()
  for(i in 1:length(cor_res)){
    
    cor_data_one <- cor_res[[i]]
    #we want to set NA to lower tri so we don't replicate results.
    cor_data_one[lower.tri(cor_data_one)] <- NA
  
    melt_res <- melt(cor_data_one)
  
    ##remove rows that are NA or that are from the same tool.
    removal_rows <- which(melt_res$Var1 == melt_res$Var2)
  
    melt_res <- melt_res[-removal_rows,]
  
    removal_rows <- which(is.na(melt_res$value))
  
    melt_res <- melt_res[-removal_rows,]
    
    if(i == 1){
      res_results <- melt_res
    }else{
      
      res_results <- rbind(res_results, melt_res)
    }
  }
  return(res_results)
}
```

## Analysis

```{r}
filt_cor_results <- get_tool_correlations(filt_results, method="spearman")

filt_cor_df <- get_melted_cor_df(filt_cor_results)
filt_cor_df$comp <- paste(filt_cor_df$Var1, filt_cor_df$Var2, sep="_")


mean_values <- aggregate(x=filt_cor_df$value, by=list(filt_cor_df$Var1, filt_cor_df$Var2), FUN=mean)
mean_values$comp <- paste(mean_values$Group.1, mean_values$Group.2, sep="_")

filt_cor_df$mean_values <- mean_values$x[match(filt_cor_df$comp, mean_values$comp)]

#remove ancom
ancom_values <- which(filt_cor_df$Var1=='ancom' | filt_cor_df$Var2=='ancom')

remove_ancom <- filt_cor_df[-ancom_values,]

remove_ancom$Var1 <- tool_names[remove_ancom$Var1]
remove_ancom$Var2 <- tool_names[remove_ancom$Var2]

#heatmap with boxplots
test_box <- ggplot(data=remove_ancom, aes(y=value, fill=mean_values))   + 
  geom_rect(xmin= -Inf, xmax=Inf, ymin=-Inf,ymax=Inf) + geom_boxplot() + facet_grid(Var2 ~ Var1, scales="free", space="free") + theme_bw() +
  scale_y_continuous(limits=c(-1,1)) + geom_hline(yintercept=0, alpha=0.1) + ylab("Spearman's Rho") + scale_x_discrete(element_blank()) +
  scale_fill_gradient2(low="blue", mid="white", high="red", name="Spearman's Rho")
test_box

saveRDS()



#heatmap

test <- ggplot(remove_ancom_pearson_r_filt, aes(x=Group.1, y=Group.2, fill=x, label=round(x,2))) + geom_tile() + geom_text(check_overlap = T)  + 
  geom_rect(xmin= -Inf, xmax=Inf, ymin=-Inf,ymax=Inf) +
  scale_fill_gradient(low="white", high="red", name="Mean Pearson's r") + coord_flip() + theme_minimal() +xlab('') + ylab('')
test
#okay we have 38 different matrices.. we are interested in the mean value for each correlation...
#lets first try melting it all 


```